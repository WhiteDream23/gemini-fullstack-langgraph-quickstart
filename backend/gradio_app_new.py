import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import gradio as gr
import asyncio
from langchain_core.messages import HumanMessage
from agent.graph import graph
from agent.knowledge_base import knowledge_base
import json
from typing import List, Dict, Any
import time
import tempfile
from pathlib import Path


class ResearchAgent:
    def __init__(self):
        self.conversation_history = []
    
    def format_sources(self, sources: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æœç´¢æ¥æº"""
        if not sources:
            return "æ— å¯ç”¨æ¥æº"
        
        formatted_sources = []
        for i, source in enumerate(sources, 1):
            title = source.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = source.get('value', source.get('url', ''))
            content_preview = source.get('content', '')[:200] + "..." if source.get('content') else ""
            
            formatted_sources.append(f"""
**æ¥æº {i}: {title}**
- é“¾æ¥: {url}
- æ‘˜è¦: {content_preview}
""")
        
        return "\n".join(formatted_sources)
    
    def research_query(self, question: str, initial_queries: int = 3, max_loops: int = 2, reasoning_model: str = "Qwen/Qwen3-30B-A3B-Instruct-2507"):
        """æ‰§è¡Œç ”ç©¶æŸ¥è¯¢"""
        if not question.strip():
            return "è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„é—®é¢˜ã€‚", "", "", ""
        
        try:
            # å‡†å¤‡è¾“å…¥çŠ¶æ€
            state = {
                "messages": [HumanMessage(content=question)],
                "initial_search_query_count": initial_queries,
                "max_research_loops": max_loops,
                "reasoning_model": reasoning_model,
            }
            
            # æ‰§è¡Œå›¾
            result = graph.invoke(state)
            
            # æå–ç»“æœ
            messages = result.get("messages", [])
            sources = result.get("sources_gathered", [])
            search_queries = result.get("search_query", [])
            rag_result = result.get("rag_result", "")
            use_local_knowledge = result.get("use_local_knowledge", False)
            
            # è·å–æœ€ç»ˆç­”æ¡ˆ
            final_answer = ""
            if messages:
                final_answer = messages[-1].content
            
            # æ ¼å¼åŒ–æ¥æº
            formatted_sources = self.format_sources(sources)
            
            # æ ¼å¼åŒ–æœç´¢æŸ¥è¯¢
            formatted_queries = ""
            if search_queries:
                formatted_queries = "æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢:\n" + "\n".join([f"â€¢ {query}" for query in search_queries])
            
            # æ ¼å¼åŒ– RAG ä¿¡æ¯
            rag_info = ""
            if use_local_knowledge:
                rag_info = f"âœ… åŸºäºæœ¬åœ°çŸ¥è¯†åº“å›ç­”\n\næœ¬åœ°æ£€ç´¢ç»“æœ:\n{rag_result[:500]}{'...' if len(rag_result) > 500 else ''}"
            elif rag_result:
                rag_info = f"â„¹ï¸ æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢:\n{rag_result[:300]}{'...' if len(rag_result) > 300 else ''}\n\nğŸŒ ç»§ç»­ç½‘ç»œæœç´¢è·å–æ›´å¤šä¿¡æ¯"
            
            # ä¿å­˜åˆ°å¯¹è¯å†å²
            self.conversation_history.append({
                "question": question,
                "answer": final_answer,
                "sources": sources,
                "rag_used": use_local_knowledge,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            return final_answer, formatted_sources, formatted_queries, rag_info
            
        except Exception as e:
            error_msg = f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
            return error_msg, "", "", ""
    
    def upload_document(self, file):
        """ä¸Šä¼ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if file is None:
            return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶"
        
        try:
            # è·å–æ–‡ä»¶è·¯å¾„
            file_path = Path(file.name)
            
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            supported_types = ['.md', '.txt', '.pdf', '.docx']
            if file_path.suffix.lower() not in supported_types:
                return f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}ã€‚æ”¯æŒçš„ç±»å‹: {', '.join(supported_types)}"
            
            # å¤åˆ¶æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•
            knowledge_dir = knowledge_base.knowledge_dir
            target_path = knowledge_dir / file_path.name
            
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            target_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ–‡ä»¶
            import shutil
            shutil.copy2(file_path, target_path)
            
            # é‡å»ºå‘é‡ç´¢å¼•
            knowledge_base.build_vector_store(force_rebuild=True)
            
            return f"âœ… æ–‡ä»¶ '{file_path.name}' å·²æˆåŠŸæ·»åŠ åˆ°çŸ¥è¯†åº“å¹¶é‡å»ºç´¢å¼•"
            
        except Exception as e:
            return f"âŒ ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}"
    
    def search_knowledge_base(self, query: str, top_k: int = 5):
        """æœç´¢æœ¬åœ°çŸ¥è¯†åº“"""
        if not query.strip():
            return "è¯·è¾“å…¥æœç´¢æŸ¥è¯¢"
        
        try:
            results = knowledge_base.search(query, top_k=top_k)
            
            if not results:
                return "âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ"
            
            formatted_results = []
            for i, result in enumerate(results, 1):
                content = result['content']
                metadata = result.get('metadata', {})
                source = metadata.get('source_file', 'æœªçŸ¥æ¥æº')
                similarity = result.get('similarity', 0)
                
                formatted_results.append(f"""
**ã€ç»“æœ {i}ã€‘** (ç›¸ä¼¼åº¦: {similarity:.3f})
**æ¥æº:** {source}
**å†…å®¹:** {content[:300]}{'...' if len(content) > 300 else ''}

---
""")
            
            return f"ğŸ” æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:\n\n{''.join(formatted_results)}"
            
        except Exception as e:
            return f"âŒ æœç´¢å¤±è´¥: {str(e)}"
    
    def get_knowledge_base_stats(self):
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = knowledge_base.get_stats()
            
            info = f"""
ğŸ“Š **çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯**

- **æ€»æ–‡æ¡£å—æ•°:** {stats['total_chunks']}
- **ç´¢å¼•å¤§å°:** {stats['index_size']}
- **çŸ¥è¯†åº“ç›®å½•:** {stats['knowledge_dir']}
- **å‘é‡å­˜å‚¨ç›®å½•:** {stats['vector_store_path']}

"""
            
            # æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•ä¸­çš„æ–‡ä»¶
            knowledge_dir = Path(stats['knowledge_dir'])
            if knowledge_dir.exists():
                files = list(knowledge_dir.rglob("*"))
                doc_files = [f for f in files if f.is_file() and f.suffix.lower() in ['.md', '.txt', '.pdf', '.docx']]
                
                info += f"- **çŸ¥è¯†åº“æ–‡ä»¶æ•°:** {len(doc_files)}\n\n"
                
                if doc_files:
                    info += "**æ–‡ä»¶åˆ—è¡¨:**\n"
                    for file in doc_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                        info += f"  - {file.name}\n"
                    if len(doc_files) > 10:
                        info += f"  - ... å’Œå…¶ä»– {len(doc_files) - 10} ä¸ªæ–‡ä»¶\n"
            
            return info
            
        except Exception as e:
            return f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}"
    
    def rebuild_index(self):
        """é‡å»ºçŸ¥è¯†åº“ç´¢å¼•"""
        try:
            knowledge_base.build_vector_store(force_rebuild=True)
            return "âœ… çŸ¥è¯†åº“ç´¢å¼•é‡å»ºå®Œæˆ"
        except Exception as e:
            return f"âŒ é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}"
    
    def get_conversation_history(self):
        """è·å–å¯¹è¯å†å²"""
        if not self.conversation_history:
            return "æš‚æ— å¯¹è¯å†å²"
        
        history_text = []
        for i, conv in enumerate(self.conversation_history, 1):
            rag_badge = "ğŸ”— RAG" if conv.get('rag_used', False) else "ğŸŒ WEB"
            history_text.append(f"""
**å¯¹è¯ {i} ({conv['timestamp']})** {rag_badge}
**é—®é¢˜:** {conv['question']}
**å›ç­”:** {conv['answer'][:300]}{'...' if len(conv['answer']) > 300 else ''}
---
""")
        
        return "\n".join(history_text)


# åˆ›å»ºç ”ç©¶ä»£ç†å®ä¾‹
agent = ResearchAgent()


def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="Qwen æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ + æœ¬åœ°çŸ¥è¯†åº“",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1400px !important;
        }
        .main-header {
            text-align: center;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 20px;
        }
        """
    ) as demo:
        
        gr.HTML("""
        <div class="main-header">
            ğŸ” Qwen æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ + ğŸ“š æœ¬åœ°çŸ¥è¯†åº“
        </div>
        <p style="text-align: center; font-size: 1.1em; color: #666;">
            åŸºäº Qwen æ¨¡å‹ã€Tavily æœç´¢å’Œæœ¬åœ° RAG çš„æ™ºèƒ½ç ”ç©¶å·¥å…·ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“ï¼Œå¿…è¦æ—¶è¡¥å……ç½‘ç»œæœç´¢
        </p>
        """)
        
        with gr.Tab("ğŸ’­ æ™ºèƒ½é—®ç­”"):
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿäººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹ï¼Ÿ",
                        lines=3,
                        max_lines=5
                    )
                    
                    with gr.Row():
                        submit_btn = gr.Button("ğŸš€ å¼€å§‹ç ”ç©¶", variant="primary", size="lg")
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ é«˜çº§è®¾ç½®")
                    initial_queries = gr.Slider(
                        label="åˆå§‹æŸ¥è¯¢æ•°é‡",
                        minimum=1,
                        maximum=5,
                        value=3,
                        step=1,
                        info="ç”Ÿæˆçš„åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡"
                    )
                    max_loops = gr.Slider(
                        label="æœ€å¤§ç ”ç©¶å¾ªç¯",
                        minimum=1,
                        maximum=5,
                        value=2,
                        step=1,
                        info="æœ€å¤§åæ€å’Œè¡¥å……æœç´¢æ¬¡æ•°"
                    )
                    reasoning_model = gr.Dropdown(
                        label="æ¨ç†æ¨¡å‹",
                        choices=["Qwen/Qwen3-30B-A3B-Instruct-2507"],
                        value="Qwen/Qwen3-30B-A3B-Instruct-2507",
                        info="ç”¨äºåˆ†æå’Œæ¨ç†çš„æ¨¡å‹"
                    )
            
            gr.Markdown("---")
            
            with gr.Row():
                with gr.Column():
                    answer_output = gr.Textbox(
                        label="ğŸ“ ç ”ç©¶ç»“æœ",
                        lines=15,
                        max_lines=20,
                        show_copy_button=True
                    )
                
                with gr.Column():
                    with gr.Tab("ğŸ“š ä¿¡æ¯æ¥æº"):
                        sources_output = gr.Textbox(
                            label="ç½‘ç»œæœç´¢æ¥æº",
                            lines=8,
                            max_lines=12,
                            show_copy_button=True
                        )
                    
                    with gr.Tab("ğŸ” æœç´¢è®°å½•"):
                        queries_output = gr.Textbox(
                            label="æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢",
                            lines=5,
                            max_lines=8
                        )
                    
                    with gr.Tab("ğŸ”— RAG ä¿¡æ¯"):
                        rag_output = gr.Textbox(
                            label="æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢",
                            lines=8,
                            max_lines=12,
                            show_copy_button=True
                        )
        
        with gr.Tab("ğŸ“š çŸ¥è¯†åº“ç®¡ç†"):
            with gr.Row():
                with gr.Column():
                    gr.Markdown("### ğŸ“¤ ä¸Šä¼ æ–‡æ¡£")
                    file_upload = gr.File(
                        label="é€‰æ‹©æ–‡ä»¶",
                        file_types=[".md", ".txt", ".pdf", ".docx"],
                        type="filepath"
                    )
                    upload_btn = gr.Button("ğŸ“ ä¸Šä¼ åˆ°çŸ¥è¯†åº“", variant="primary")
                    upload_result = gr.Textbox(label="ä¸Šä¼ ç»“æœ", lines=3)
                
                with gr.Column():
                    gr.Markdown("### ğŸ” æœç´¢çŸ¥è¯†åº“")
                    search_input = gr.Textbox(
                        label="æœç´¢æŸ¥è¯¢",
                        placeholder="è¾“å…¥è¦æœç´¢çš„å†…å®¹..."
                    )
                    search_topk = gr.Slider(
                        label="è¿”å›ç»“æœæ•°é‡",
                        minimum=1,
                        maximum=10,
                        value=5,
                        step=1
                    )
                    search_btn = gr.Button("ğŸ” æœç´¢", variant="primary")
            
            with gr.Row():
                with gr.Column():
                    search_results = gr.Textbox(
                        label="ğŸ” æœç´¢ç»“æœ",
                        lines=15,
                        max_lines=20,
                        show_copy_button=True
                    )
                
                with gr.Column():
                    gr.Markdown("### âš™ï¸ çŸ¥è¯†åº“ç®¡ç†")
                    
                    stats_btn = gr.Button("ğŸ“Š æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯", variant="secondary")
                    rebuild_btn = gr.Button("ğŸ”„ é‡å»ºç´¢å¼•", variant="secondary")
                    
                    kb_stats = gr.Textbox(
                        label="ğŸ“Š çŸ¥è¯†åº“ä¿¡æ¯",
                        lines=12,
                        max_lines=15
                    )
        
        with gr.Tab("ğŸ“œ å¯¹è¯å†å²"):
            history_display = gr.Textbox(
                label="å¯¹è¯è®°å½•",
                lines=20,
                max_lines=25,
                show_copy_button=True
            )
            refresh_history_btn = gr.Button("ğŸ”„ åˆ·æ–°å†å²è®°å½•", variant="secondary")
        
        with gr.Tab("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
            gr.Markdown("""
            ## ğŸŒŸ åŠŸèƒ½ç‰¹ç‚¹
            
            ### ğŸ¤– æ™ºèƒ½ç ”ç©¶æµç¨‹
            1. **æœ¬åœ°ä¼˜å…ˆ:** é¦–å…ˆæœç´¢æœ¬åœ°çŸ¥è¯†åº“
            2. **æ™ºèƒ½åˆ¤æ–­:** è¯„ä¼°æœ¬åœ°ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå›ç­”é—®é¢˜
            3. **ç½‘ç»œè¡¥å……:** å¿…è¦æ—¶è¿›è¡Œç½‘ç»œæœç´¢è·å–æœ€æ–°ä¿¡æ¯
            4. **ç»¼åˆå›ç­”:** æ•´åˆæœ¬åœ°å’Œç½‘ç»œä¿¡æ¯æä¾›å…¨é¢ç­”æ¡ˆ
            
            ### ğŸ“š æœ¬åœ°çŸ¥è¯†åº“
            - **æ–‡æ¡£æ”¯æŒ:** Markdownã€TXTã€PDFã€DOCX
            - **å‘é‡æ£€ç´¢:** åŸºäº FAISS çš„é«˜æ•ˆè¯­ä¹‰æœç´¢
            - **å®æ—¶æ›´æ–°:** ä¸Šä¼ æ–‡æ¡£åè‡ªåŠ¨é‡å»ºç´¢å¼•
            - **è¯­ä¹‰ç†è§£:** ä½¿ç”¨ Sentence Transformers è¿›è¡ŒåµŒå…¥
            
            ### ğŸ” æœç´¢èƒ½åŠ›
            - **å®æ—¶ç½‘ç»œæœç´¢:** è·å–æœ€æ–°ä¿¡æ¯
            - **å¤šæºéªŒè¯:** äº¤å‰éªŒè¯ä¿¡æ¯å‡†ç¡®æ€§
            - **æ™ºèƒ½ç­›é€‰:** è¿‡æ»¤é«˜è´¨é‡å†…å®¹
            - **æ¥æºè¿½æº¯:** æä¾›è¯¦ç»†å¼•ç”¨é“¾æ¥
            
            ### ğŸ’¡ ä½¿ç”¨æŠ€å·§
            1. **æ„å»ºçŸ¥è¯†åº“:** ä¸Šä¼ ç›¸å…³æ–‡æ¡£å»ºç«‹ä¸“ä¸šçŸ¥è¯†åº“
            2. **æ˜ç¡®é—®é¢˜:** é—®é¢˜è¶Šå…·ä½“ï¼Œç­”æ¡ˆè¶Šç²¾å‡†
            3. **æŸ¥çœ‹ RAG ä¿¡æ¯:** äº†è§£ç­”æ¡ˆæ¥æºï¼ˆæœ¬åœ° vs ç½‘ç»œï¼‰
            4. **ç®¡ç†çŸ¥è¯†åº“:** å®šæœŸæ›´æ–°å’Œç»´æŠ¤çŸ¥è¯†åº“å†…å®¹
            
            ### ğŸ“ ç¤ºä¾‹é—®é¢˜
            - "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"ï¼ˆå¦‚æœçŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³æ–‡æ¡£ï¼‰
            - "2024å¹´äººå·¥æ™ºèƒ½çš„æœ€æ–°çªç ´æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼ˆç½‘ç»œæœç´¢ï¼‰
            - "æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"ï¼ˆæœ¬åœ°çŸ¥è¯†åº“ï¼‰
            - "å¯å†ç”Ÿèƒ½æºæŠ€æœ¯çš„å‘å±•è¶‹åŠ¿å¦‚ä½•ï¼Ÿ"ï¼ˆæ··åˆæœç´¢ï¼‰
            
            ### ğŸ“š çŸ¥è¯†åº“ç®¡ç†
            - **ä¸Šä¼ æ–‡æ¡£:** æ”¯æŒå¤šç§æ ¼å¼çš„æ–‡æ¡£ä¸Šä¼ 
            - **æœç´¢æµ‹è¯•:** æµ‹è¯•çŸ¥è¯†åº“çš„æ£€ç´¢æ•ˆæœ
            - **ç»Ÿè®¡ä¿¡æ¯:** æŸ¥çœ‹çŸ¥è¯†åº“çš„è§„æ¨¡å’ŒçŠ¶æ€
            - **é‡å»ºç´¢å¼•:** åœ¨æ·»åŠ å¤§é‡æ–‡æ¡£åé‡å»ºç´¢å¼•
            
            ### âš ï¸ æ³¨æ„äº‹é¡¹
            - ç¡®ä¿å·²é…ç½® `OPENAI_API_KEY` å’Œ `TAVILY_API_KEY`
            - æœ¬åœ°çŸ¥è¯†åº“ä¼˜å…ˆï¼Œå¯èƒ½ä¸åŒ…å«æœ€æ–°ä¿¡æ¯
            - å¤§æ–‡æ¡£å»ºè®®åˆ†æ®µä¸Šä¼ ä»¥æé«˜æ£€ç´¢ç²¾åº¦
            - å®šæœŸå¤‡ä»½çŸ¥è¯†åº“æ–‡ä»¶å’Œç´¢å¼•
            """)
        
        # äº‹ä»¶ç»‘å®š - æ™ºèƒ½é—®ç­”
        submit_btn.click(
            fn=agent.research_query,
            inputs=[question_input, initial_queries, max_loops, reasoning_model],
            outputs=[answer_output, sources_output, queries_output, rag_output],
            show_progress=True
        )
        
        question_input.submit(
            fn=agent.research_query,
            inputs=[question_input, initial_queries, max_loops, reasoning_model],
            outputs=[answer_output, sources_output, queries_output, rag_output],
            show_progress=True
        )
        
        clear_btn.click(
            fn=lambda: ("", "", "", "", ""),
            outputs=[question_input, answer_output, sources_output, queries_output, rag_output]
        )
        
        # äº‹ä»¶ç»‘å®š - çŸ¥è¯†åº“ç®¡ç†
        upload_btn.click(
            fn=agent.upload_document,
            inputs=[file_upload],
            outputs=[upload_result],
            show_progress=True
        )
        
        search_btn.click(
            fn=agent.search_knowledge_base,
            inputs=[search_input, search_topk],
            outputs=[search_results],
            show_progress=True
        )
        
        stats_btn.click(
            fn=agent.get_knowledge_base_stats,
            outputs=[kb_stats]
        )
        
        rebuild_btn.click(
            fn=agent.rebuild_index,
            outputs=[kb_stats],
            show_progress=True
        )
        
        # äº‹ä»¶ç»‘å®š - å¯¹è¯å†å²
        refresh_history_btn.click(
            fn=agent.get_conversation_history,
            outputs=[history_display]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        quiet=False
    )
