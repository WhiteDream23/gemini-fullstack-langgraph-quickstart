#!/usr/bin/env python3
"""
çŸ¥è¯†åº“ç®¡ç†è„šæœ¬
ç”¨äºç®¡ç†æœ¬åœ°çŸ¥è¯†åº“ï¼šæ·»åŠ æ–‡æ¡£ã€æ„å»ºç´¢å¼•ã€æŸ¥è¯¢æµ‹è¯•ç­‰
"""

import os
import sys
import argparse
from pathlib import Path

# æ·»åŠ æºä»£ç è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, 'src')
sys.path.insert(0, src_path)

from agent.knowledge_base import knowledge_base


def add_document(file_path: str):
    """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
    file_path = Path(file_path)
    if not file_path.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    try:
        # å¤åˆ¶æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•
        knowledge_dir = knowledge_base.knowledge_dir
        target_path = knowledge_dir / file_path.name
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        target_path.parent.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶
        import shutil
        shutil.copy2(file_path, target_path)
        
        print(f"âœ… æ–‡ä»¶å·²æ·»åŠ åˆ°çŸ¥è¯†åº“: {target_path}")
        
        # é‡å»ºå‘é‡ç´¢å¼•
        print("ğŸ”„ é‡å»ºå‘é‡ç´¢å¼•...")
        knowledge_base.build_vector_store(force_rebuild=True)
        print("âœ… å‘é‡ç´¢å¼•é‡å»ºå®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")


def build_index():
    """æ„å»ºå‘é‡ç´¢å¼•"""
    try:
        print("ğŸ”„ å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
        knowledge_base.build_vector_store(force_rebuild=True)
        print("âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆ")
    except Exception as e:
        print(f"âŒ æ„å»ºç´¢å¼•å¤±è´¥: {e}")


def search_knowledge(query: str, top_k: int = 5):
    """æœç´¢çŸ¥è¯†åº“"""
    try:
        print(f"ğŸ” æœç´¢: {query}")
        results = knowledge_base.search(query, top_k=top_k)
        
        if not results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
            return
        
        print(f"\nğŸ“š æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
        for i, result in enumerate(results, 1):
            content = result['content']
            metadata = result.get('metadata', {})
            source = metadata.get('source_file', 'æœªçŸ¥æ¥æº')
            similarity = result.get('similarity', 0)
            
            print(f"\nã€ç»“æœ {i}ã€‘")
            print(f"æ¥æº: {source}")
            print(f"ç›¸ä¼¼åº¦: {similarity:.3f}")
            print(f"å†…å®¹: {content[:200]}{'...' if len(content) > 200 else ''}")
            print("-" * 50)
            
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {e}")


def show_stats():
    """æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = knowledge_base.get_stats()
        print("ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æ–‡æ¡£å—æ•°: {stats['total_chunks']}")
        print(f"  ç´¢å¼•å¤§å°: {stats['index_size']}")
        print(f"  çŸ¥è¯†åº“ç›®å½•: {stats['knowledge_dir']}")
        print(f"  å‘é‡å­˜å‚¨ç›®å½•: {stats['vector_store_path']}")
        
        # æ£€æŸ¥çŸ¥è¯†åº“ç›®å½•ä¸­çš„æ–‡ä»¶
        knowledge_dir = Path(stats['knowledge_dir'])
        if knowledge_dir.exists():
            files = list(knowledge_dir.rglob("*"))
            doc_files = [f for f in files if f.is_file() and f.suffix.lower() in ['.md', '.txt', '.pdf', '.docx']]
            print(f"  çŸ¥è¯†åº“æ–‡ä»¶æ•°: {len(doc_files)}")
            
            if doc_files:
                print("  æ–‡ä»¶åˆ—è¡¨:")
                for file in doc_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªæ–‡ä»¶
                    print(f"    - {file.name}")
                if len(doc_files) > 10:
                    print(f"    ... å’Œå…¶ä»– {len(doc_files) - 10} ä¸ªæ–‡ä»¶")
        
    except Exception as e:
        print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")


def create_sample_docs():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    knowledge_dir = knowledge_base.knowledge_dir
    
    sample_docs = [
        {
            "filename": "ai_introduction.md",
            "content": """# äººå·¥æ™ºèƒ½ç®€ä»‹

## ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½

äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯æŒ‡è®©æœºå™¨è¡¨ç°å‡ºäººç±»æ™ºèƒ½ç‰¹å¾çš„æŠ€æœ¯å’Œæ–¹æ³•ã€‚

## ä¸»è¦åˆ†ç±»

### 1. æœºå™¨å­¦ä¹ 
æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚

### 2. æ·±åº¦å­¦ä¹ 
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒæ¨¡ä»¿äººè„‘ç¥ç»ç½‘ç»œçš„ç»“æ„å’ŒåŠŸèƒ½ã€‚

### 3. è‡ªç„¶è¯­è¨€å¤„ç†
è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚

## åº”ç”¨é¢†åŸŸ

- å›¾åƒè¯†åˆ«
- è¯­éŸ³è¯†åˆ«
- æ¨èç³»ç»Ÿ
- è‡ªåŠ¨é©¾é©¶
- åŒ»ç–—è¯Šæ–­
"""
        },
        {
            "filename": "machine_learning_basics.md",
            "content": """# æœºå™¨å­¦ä¹ åŸºç¡€

## ç›‘ç£å­¦ä¹ 

ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥å­¦ä¹ è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚

### å¸¸è§ç®—æ³•
- çº¿æ€§å›å½’
- é€»è¾‘å›å½’
- å†³ç­–æ ‘
- éšæœºæ£®æ—
- æ”¯æŒå‘é‡æœº

## æ— ç›‘ç£å­¦ä¹ 

æ— ç›‘ç£å­¦ä¹ åœ¨æ²¡æœ‰æ ‡è®°æ•°æ®çš„æƒ…å†µä¸‹å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼ã€‚

### å¸¸è§ç®—æ³•
- K-meansèšç±»
- å±‚æ¬¡èšç±»
- ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
- DBSCAN

## å¼ºåŒ–å­¦ä¹ 

å¼ºåŒ–å­¦ä¹ é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚

### æ ¸å¿ƒæ¦‚å¿µ
- æ™ºèƒ½ä½“ï¼ˆAgentï¼‰
- ç¯å¢ƒï¼ˆEnvironmentï¼‰
- çŠ¶æ€ï¼ˆStateï¼‰
- åŠ¨ä½œï¼ˆActionï¼‰
- å¥–åŠ±ï¼ˆRewardï¼‰
"""
        },
        {
            "filename": "deep_learning_overview.md",
            "content": """# æ·±åº¦å­¦ä¹ æ¦‚è¿°

## ç¥ç»ç½‘ç»œåŸºç¡€

äººå·¥ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œå®ƒæ¨¡ä»¿ç”Ÿç‰©ç¥ç»ç½‘ç»œçš„ç»“æ„ã€‚

### åŸºæœ¬ç»„ä»¶
- ç¥ç»å…ƒï¼ˆèŠ‚ç‚¹ï¼‰
- æƒé‡å’Œåç½®
- æ¿€æ´»å‡½æ•°
- å±‚ï¼ˆè¾“å…¥å±‚ã€éšè—å±‚ã€è¾“å‡ºå±‚ï¼‰

## å¸¸è§ç½‘ç»œæ¶æ„

### 1. å‰é¦ˆç¥ç»ç½‘ç»œ
æœ€ç®€å•çš„ç¥ç»ç½‘ç»œç±»å‹ï¼Œä¿¡æ¯åªå‘å‰ä¼ æ’­ã€‚

### 2. å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
ä¸»è¦ç”¨äºå›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚

### 3. å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
é€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚

### 4. å˜å‹å™¨ï¼ˆTransformerï¼‰
ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸»æµæ¶æ„ã€‚

## è®­ç»ƒè¿‡ç¨‹

1. å‰å‘ä¼ æ’­
2. è®¡ç®—æŸå¤±
3. åå‘ä¼ æ’­
4. å‚æ•°æ›´æ–°
"""
        }
    ]
    
    try:
        for doc in sample_docs:
            file_path = knowledge_dir / doc["filename"]
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(doc["content"])
            print(f"âœ… åˆ›å»ºç¤ºä¾‹æ–‡æ¡£: {doc['filename']}")
        
        print("ğŸ”„ æ„å»ºå‘é‡ç´¢å¼•...")
        knowledge_base.build_vector_store(force_rebuild=True)
        print("âœ… ç¤ºä¾‹çŸ¥è¯†åº“åˆ›å»ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ åˆ›å»ºç¤ºä¾‹æ–‡æ¡£å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description="çŸ¥è¯†åº“ç®¡ç†å·¥å…·")
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # æ·»åŠ æ–‡æ¡£å‘½ä»¤
    add_parser = subparsers.add_parser('add', help='æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“')
    add_parser.add_argument('file_path', help='è¦æ·»åŠ çš„æ–‡ä»¶è·¯å¾„')
    
    # æ„å»ºç´¢å¼•å‘½ä»¤
    subparsers.add_parser('build', help='æ„å»ºå‘é‡ç´¢å¼•')
    
    # æœç´¢å‘½ä»¤
    search_parser = subparsers.add_parser('search', help='æœç´¢çŸ¥è¯†åº“')
    search_parser.add_argument('query', help='æœç´¢æŸ¥è¯¢')
    search_parser.add_argument('--top-k', type=int, default=5, help='è¿”å›ç»“æœæ•°é‡')
    
    # ç»Ÿè®¡ä¿¡æ¯å‘½ä»¤
    subparsers.add_parser('stats', help='æ˜¾ç¤ºçŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯')
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£å‘½ä»¤
    subparsers.add_parser('init', help='åˆ›å»ºç¤ºä¾‹çŸ¥è¯†åº“')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    print("ğŸš€ çŸ¥è¯†åº“ç®¡ç†å·¥å…·")
    print("-" * 40)
    
    if args.command == 'add':
        add_document(args.file_path)
    elif args.command == 'build':
        build_index()
    elif args.command == 'search':
        search_knowledge(args.query, args.top_k)
    elif args.command == 'stats':
        show_stats()
    elif args.command == 'init':
        create_sample_docs()


if __name__ == "__main__":
    main()
