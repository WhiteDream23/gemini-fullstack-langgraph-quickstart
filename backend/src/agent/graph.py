import os
import asyncio
from agent.tools_and_schemas import SearchQueryList, Reflection,ClarifyWithUser,ResearchQuestion
from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage
from langgraph.types import Send
from langgraph.graph import StateGraph
from langgraph.graph import START, END
from langchain_core.runnables import RunnableConfig
from tavily import TavilyClient
from typing_extensions import Literal
from langgraph.prebuilt import create_react_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import Command,interrupt
from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
    RAGState,
)
from agent.configuration import Configuration
from agent.prompts import (
    query_writer_instructions,
    web_searcher_instructions,
    reflection_instructions,
    answer_instructions,
    clarify_with_user_instructions,
    transform_messages_into_research_topic_prompt,
)

from langchain_openai import ChatOpenAI
from agent.utils import (
    get_research_topic,
    get_today_str,
    get_buffer_string,
)

from agent.agent_tools import create_rag_tool, evaluate_rag_sufficiency,save_file

load_dotenv()

if os.getenv("OPENAI_API_KEY") is None:
    raise ValueError("OPENAI_API_KEY is not set")

if os.getenv("TAVILY_API_KEY") is None:
    raise ValueError("TAVILY_API_KEY is not set")

# Initialize Tavily client
tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

# Initialize memory for the RAG agent
memory = MemorySaver()


# Nodes

def clarify_with_user(state: OverallState, config: RunnableConfig)  -> Command[Literal["write_research_brief", "__end__"]]:
    """
    ç”¨æˆ·æ„å›¾æ¾„æ¸…èŠ‚ç‚¹
    
    åˆ†æç”¨æˆ·çš„é—®é¢˜æ˜¯å¦åŒ…å«è¶³å¤Ÿä¿¡æ¯è¿›è¡Œç ”ç©¶ï¼Œ
    å¦‚æœä¸å¤Ÿæ˜ç¡®åˆ™ç”Ÿæˆæ¾„æ¸…é—®é¢˜ï¼Œå¦åˆ™ç¡®è®¤ç†è§£ã€‚
    """
    configurable = Configuration.from_runnable_config(config)
    
    # åˆå§‹åŒ–æ¨¡å‹
    llm = ChatOpenAI(
        model=configurable.reasoning_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
    )
    
    # è®¾ç½®ç»“æ„åŒ–è¾“å‡º
    structured_output_model = llm.with_structured_output(ClarifyWithUser)
    
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œæ„å›¾æ¾„æ¸…
    response = structured_output_model.invoke([
        HumanMessage(content=clarify_with_user_instructions.format(
            messages=get_buffer_string(messages=state.get("messages", [])), 
            date=get_today_str()
        ))
    ])
    
    # æ›´æ–°çŠ¶æ€
    if response.need_clarification:
        return Command(
            goto=END, 
            update={"messages": [AIMessage(content=response.question)],
                    "need_clarification": True,}
        )
    else:
        return Command(
            goto="write_research_brief", 
            update={"messages": [AIMessage(content=response.verification)],
                     "need_clarification": False}
        )


def write_research_brief(state: OverallState, config: RunnableConfig) -> OverallState:
    """
    ç ”ç©¶ç®€æŠ¥ç”ŸæˆèŠ‚ç‚¹
    
    å°†å¯¹è¯å†å²è½¬æ¢ä¸ºè¯¦ç»†çš„ç ”ç©¶ç®€æŠ¥ï¼Œ
    ä¸ºåç»­çš„RAGæ£€ç´¢å’Œç½‘ç»œæœç´¢æä¾›æ˜ç¡®æŒ‡å¯¼ã€‚
    """
    configurable = Configuration.from_runnable_config(config)
    
    # åˆå§‹åŒ–æ¨¡å‹
    llm = ChatOpenAI(
        model=configurable.reasoning_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
    )
    
    # è®¾ç½®ç»“æ„åŒ–è¾“å‡º
    structured_output_model = llm.with_structured_output(ResearchQuestion)
    
    # ç”Ÿæˆç ”ç©¶ç®€æŠ¥
    response = structured_output_model.invoke([
        HumanMessage(content=transform_messages_into_research_topic_prompt.format(
            messages=get_buffer_string(state.get("messages", [])),
            date=get_today_str()
        ))
    ])
    
    # æ›´æ–°çŠ¶æ€
    return {
        "research_brief": response.research_brief,
        "supervisor_messages": [HumanMessage(content=f"ç ”ç©¶ç®€æŠ¥ï¼š{response.research_brief}")],
        "messages": [AIMessage(content=f"ğŸ“‹ ç ”ç©¶ç®€æŠ¥å·²ç”Ÿæˆï¼š\n\n{response.research_brief}")]
    }


def local_rag_search(state: OverallState, config: RunnableConfig) -> RAGState:
    """æœ¬åœ° RAG æ£€ç´¢èŠ‚ç‚¹
    
    é¦–å…ˆåœ¨æœ¬åœ°çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯ï¼Œå¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œ
    åˆ™å¯ä»¥ç›´æ¥åŸºäºæœ¬åœ°çŸ¥è¯†å›ç­”ï¼Œå¦åˆ™ç»§ç»­ç½‘ç»œæœç´¢æµç¨‹ã€‚
    """
    configurable = Configuration.from_runnable_config(config)
    
    # è·å–ç”¨æˆ·é—®é¢˜
    user_question = get_research_topic(state["research_brief"])
    
    # åˆ›å»º LLM å®ä¾‹
    llm = ChatOpenAI(
        model=configurable.query_generator_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
    )
    
    # åˆ›å»º RAG å·¥å…·
    retrieve_tool = create_rag_tool()
    
    # åˆ›å»º React Agent
    agent_executor = create_react_agent(llm, [retrieve_tool])
    
    try:
        # æ‰§è¡Œ RAG æ£€ç´¢
        rag_prompt = f"""è¯·ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢å·¥å…·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{user_question}
        
å¦‚æœæœ¬åœ°çŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·åŸºäºæ£€ç´¢åˆ°çš„å†…å®¹æä¾›è¯¦ç»†å›ç­”ã€‚
å¦‚æœæœ¬åœ°çŸ¥è¯†åº“ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜éœ€è¦è¿›ä¸€æ­¥çš„ç½‘ç»œæœç´¢ã€‚"""
        
        result = agent_executor.invoke(
            {"messages": [HumanMessage(content=rag_prompt)]},
            config={"configurable": {"thread_id": "rag_search"}}
        )
        
        # æå–ç»“æœ
        rag_result = ""
        if result.get("messages"):
            rag_result = result["messages"][-1].content
        
        # è¯„ä¼° RAG ç»“æœçš„å……åˆ†æ€§
        evaluation = evaluate_rag_sufficiency(rag_result, user_question)
        
        return {
            "rag_result": rag_result,
            "rag_sufficient": evaluation["is_sufficient"],
            "rag_confidence": evaluation["confidence"],
            "evaluation_reason": evaluation["reason"],
            "use_local_knowledge": evaluation["is_sufficient"]
        }
        
    except Exception as e:
        return {
            "rag_result": f"æœ¬åœ° RAG æ£€ç´¢å¤±è´¥: {str(e)}",
            "rag_sufficient": False,
            "rag_confidence": 0.0,
            "evaluation_reason": "RAG æ£€ç´¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
            "use_local_knowledge": False
        }


def evaluate_rag_result(state: OverallState) -> str:
    """è¯„ä¼° RAG ç»“æœï¼Œå†³å®šä¸‹ä¸€æ­¥æµç¨‹"""
    if state.get("rag_sufficient", False) and state.get("rag_confidence", 0) > 0.5:
        return "finalize_answer_with_rag"
    else:
        return "generate_query"


def finalize_answer_with_rag(state: OverallState, config: RunnableConfig):
    """åŸºäº RAG ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model
    
    # è·å–ç”¨æˆ·é—®é¢˜å’Œ RAG ç»“æœ
    user_question = get_research_topic(state["research_brief"])
    rag_content = state.get("rag_result", "")
    
    # åˆ›å»ºåŸºäº RAG çš„å›ç­”æç¤º
    rag_answer_prompt = f"""
åŸºäºä»¥ä¸‹æœ¬åœ°çŸ¥è¯†åº“çš„æ£€ç´¢ç»“æœï¼Œè¯·ä¸ºç”¨æˆ·é—®é¢˜æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚

ç”¨æˆ·é—®é¢˜: {user_question}

æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç»“æœ:
{rag_content}

è¯·æ³¨æ„:
1. åŸºäºæä¾›çš„æœ¬åœ°çŸ¥è¯†åº“å†…å®¹è¿›è¡Œå›ç­”
2. å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯·è¯´æ˜å·²çŸ¥ä¿¡æ¯çš„å±€é™æ€§
3. ä¿æŒå›ç­”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§
4. é€‚å½“å¼•ç”¨æ¥æºä¿¡æ¯

å½“å‰æ—¥æœŸ: {get_today_str()}
"""
    
    # åˆå§‹åŒ– LLM
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
    )
    
    result = llm.invoke(rag_answer_prompt)
    
    # æ·»åŠ  RAG æ¥æºæ ‡æ³¨
    final_answer = f"{result.content}\n\n---\nğŸ’¡ æ­¤å›ç­”åŸºäºæœ¬åœ°çŸ¥è¯†åº“å†…å®¹ç”Ÿæˆ"
    
    return {
        "messages": [AIMessage(content=final_answer)],
        "sources_gathered": [],  # RAG ä¸æä¾›ç½‘ç»œæ¥æº
    }


def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """åŸºäºç”¨æˆ·é—®é¢˜ç”Ÿæˆæœç´¢æŸ¥è¯¢çš„ LangGraph èŠ‚ç‚¹ã€‚

    å½“æœ¬åœ° RAG æ£€ç´¢ä¸è¶³æ—¶ï¼Œä½¿ç”¨ Qwen æ¨¡å‹åˆ›å»ºç½‘ç»œæœç´¢æŸ¥è¯¢ã€‚

    Args:
        state: åŒ…å«ç”¨æˆ·é—®é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œå¯¹è±¡çš„é…ç½®ï¼ŒåŒ…æ‹¬ LLM æä¾›å•†è®¾ç½®

    Returns:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬å«æœ‰ç”ŸæˆæŸ¥è¯¢çš„ search_query é”®
    """
    configurable = Configuration.from_runnable_config(config)

    # check for custom initial search query count
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries

    # init Qwen model via ModelScope
    llm = ChatOpenAI(
        model=configurable.query_generator_model,
        temperature=1.0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
        extra_body={"enable_thinking": False},
    )
    structured_llm = llm.with_structured_output(SearchQueryList)

    # Format the prompt
    current_date = get_today_str()
    formatted_prompt = query_writer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["research_brief"]),
        number_queries=state["initial_search_query_count"],
    )
    # Generate the search queries
    result = structured_llm.invoke(formatted_prompt)
    return {"search_query": result.query}


def continue_to_web_research(state: QueryGenerationState):
    """å°†æœç´¢æŸ¥è¯¢å‘é€åˆ°ç½‘ç»œç ”ç©¶èŠ‚ç‚¹çš„ LangGraph èŠ‚ç‚¹ã€‚

    ç”¨äºç”Ÿæˆ n ä¸ªç½‘ç»œç ”ç©¶èŠ‚ç‚¹ï¼Œæ¯ä¸ªæœç´¢æŸ¥è¯¢å¯¹åº”ä¸€ä¸ªèŠ‚ç‚¹ã€‚
    """
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """ä½¿ç”¨ Tavily æœç´¢ API æ‰§è¡Œç½‘ç»œç ”ç©¶çš„ LangGraph èŠ‚ç‚¹ã€‚

    ä½¿ç”¨ Tavily API æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œç„¶åä½¿ç”¨ Qwen æ¨¡å‹åˆ†æ
    å’Œæ€»ç»“æœç´¢ç»“æœã€‚

    Args:
        state: åŒ…å«æœç´¢æŸ¥è¯¢å’Œç ”ç©¶å¾ªç¯è®¡æ•°çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œå¯¹è±¡çš„é…ç½®ï¼ŒåŒ…æ‹¬æœç´¢ API è®¾ç½®

    Returns:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬ sources_gatheredã€research_loop_count å’Œ web_research_results
    """
    # Configure
    configurable = Configuration.from_runnable_config(config)
    search_query = state["search_query"]
    
    try:
        # Perform web search using Tavily
        search_results = tavily_client.search(
            query=search_query,
            search_depth="advanced",
            max_results=5,
            include_domains=None,
            exclude_domains=None,
            include_answer=True,
            include_raw_content=False,
            include_images=False
        )
        
        # Extract relevant information from search results
        search_content = []
        sources_gathered = []
        
        if search_results.get("results"):
            for idx, result in enumerate(search_results["results"]):
                source_info = {
                    "short_url": f"[source_{state['id']}_{idx}]",
                    "value": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", "")
                }
                sources_gathered.append(source_info)
                
                # Format content for analysis
                search_content.append(f"æ¥æº: {result.get('title', 'Unknown')}\n"
                                    f"URL: {result.get('url', '')}\n"
                                    f"å†…å®¹: {result.get('content', '')}")
        
        # Use Qwen model to analyze and synthesize the search results
        llm = ChatOpenAI(
            model=configurable.query_generator_model,
            temperature=0,
            max_retries=2,
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
            extra_body={"enable_thinking": False},
        )
        
        # Enhanced prompt for analysis
        analysis_prompt = f"""
        {web_searcher_instructions.format(
            current_date=get_today_str(),
            research_topic=search_query,
        )}
        
        ä»¥ä¸‹æ˜¯æœç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š
        
        {chr(10).join(search_content)}
        
        è¯·åŸºäºä»¥ä¸Šæœç´¢ç»“æœï¼Œæä¾›å…³äº"{search_query}"çš„å…¨é¢åˆ†æå’Œæ€»ç»“ã€‚
        è¯·åœ¨å›ç­”ä¸­å¼•ç”¨ç›¸å…³çš„æºé“¾æ¥ï¼Œæ ¼å¼ä¸º [source_{state['id']}_X]ã€‚
        """
        
        response = llm.invoke(analysis_prompt)
        modified_text = response.content
        
        # Add Tavily's direct answer if available
        if search_results.get("answer"):
            modified_text = f"å¿«é€Ÿå›ç­”: {search_results['answer']}\n\nè¯¦ç»†åˆ†æ:\n{modified_text}"
        
    except Exception as e:
        # Fallback to knowledge-based response if Tavily fails
        print(f"Tavilyæœç´¢å¤±è´¥: {e}")
        
        llm = ChatOpenAI(
            model=configurable.query_generator_model,
            temperature=0,
            max_retries=2,
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
            extra_body={"enable_thinking": False},
        )
        
        fallback_prompt = f"""
        {web_searcher_instructions.format(
            current_date=get_today_str(),
            research_topic=search_query,
        )}
        
        æ³¨æ„ï¼šç”±äºç½‘ç»œæœç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·åŸºäºæ‚¨çš„çŸ¥è¯†åº“æä¾›å…³äº"{search_query}"çš„è¯¦ç»†ä¿¡æ¯ã€‚
        """
        
        response = llm.invoke(fallback_prompt)
        modified_text = f"[åŸºäºçŸ¥è¯†åº“å›ç­”] {response.content}"
        sources_gathered = []

    return {
        "sources_gathered": sources_gathered,
        "search_query": [search_query],
        "web_research_result": [modified_text],
    }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """è¯†åˆ«çŸ¥è¯†ç©ºç™½å¹¶ç”Ÿæˆæ½œåœ¨åç»­æŸ¥è¯¢çš„ LangGraph èŠ‚ç‚¹ã€‚

    åˆ†æå½“å‰æ‘˜è¦ä»¥è¯†åˆ«éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„é¢†åŸŸï¼Œå¹¶ç”Ÿæˆ
    æ½œåœ¨çš„åç»­æŸ¥è¯¢ã€‚ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºä»¥ JSON æ ¼å¼æå–
    åç»­æŸ¥è¯¢ã€‚

    Args:
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œç ”ç©¶ä¸»é¢˜çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œå¯¹è±¡çš„é…ç½®ï¼ŒåŒ…æ‹¬ LLM æä¾›å•†è®¾ç½®

    Returns:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬å«æœ‰ç”Ÿæˆçš„åç»­æŸ¥è¯¢çš„ search_query é”®
    """
    configurable = Configuration.from_runnable_config(config)
    # Increment the research loop count and get the reasoning model
    state["research_loop_count"] = state.get("research_loop_count", 0) + 1
    reasoning_model = state.get("reasoning_model", configurable.reflection_model)

    # Format the prompt
    current_date = get_today_str()
    formatted_prompt = reflection_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["research_brief"]),
        summaries="\n\n---\n\n".join(state["web_research_result"]),
    )
    # init Qwen Model
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=1.0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
        extra_body={"enable_thinking": False},
    )
    result = llm.with_structured_output(Reflection).invoke(formatted_prompt)

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": result.follow_up_queries,
        "research_loop_count": state["research_loop_count"],
        "number_of_ran_queries": len(state["search_query"]),
    }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """å†³å®šç ”ç©¶æµç¨‹ä¸‹ä¸€æ­¥çš„ LangGraph è·¯ç”±å‡½æ•°ã€‚

    é€šè¿‡å†³å®šæ˜¯å¦ç»§ç»­æ”¶é›†ä¿¡æ¯æˆ–åŸºäºé…ç½®çš„æœ€å¤§ç ”ç©¶å¾ªç¯æ¬¡æ•°
    å®Œæˆæ‘˜è¦æ¥æ§åˆ¶ç ”ç©¶å¾ªç¯ã€‚

    Args:
        state: åŒ…å«ç ”ç©¶å¾ªç¯è®¡æ•°çš„å½“å‰å›¾çŠ¶æ€
        config: å¯è¿è¡Œå¯¹è±¡çš„é…ç½®ï¼ŒåŒ…æ‹¬ max_research_loops è®¾ç½®

    Returns:
        æŒ‡ç¤ºä¸‹ä¸€ä¸ªè¦è®¿é—®çš„èŠ‚ç‚¹çš„å­—ç¬¦ä¸²å­—é¢é‡ï¼ˆ"web_research" æˆ– "finalize_summary"ï¼‰
    """
    configurable = Configuration.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )
    if state["is_sufficient"] or state["research_loop_count"] >= max_research_loops:
        return "finalize_answer"
    else:
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def finalize_answer(state: OverallState, config: RunnableConfig):
    """å®Œæˆç ”ç©¶æ‘˜è¦çš„ LangGraph èŠ‚ç‚¹ã€‚

    é€šè¿‡å»é‡å’Œæ ¼å¼åŒ–æ¥æºæ¥å‡†å¤‡æœ€ç»ˆè¾“å‡ºï¼Œç„¶å
    å°†å®ƒä»¬ä¸è¿è¡Œæ‘˜è¦ç»“åˆä»¥åˆ›å»ºç»“æ„è‰¯å¥½çš„
    å¸¦æœ‰é€‚å½“å¼•ç”¨çš„ç ”ç©¶æŠ¥å‘Šã€‚

    Args:
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œæ”¶é›†æ¥æºçš„å½“å‰å›¾çŠ¶æ€

    Returns:
        åŒ…å«çŠ¶æ€æ›´æ–°çš„å­—å…¸ï¼ŒåŒ…æ‹¬å«æœ‰æ ¼å¼åŒ–çš„æœ€ç»ˆæ‘˜è¦å’Œæ¥æºçš„ running_summary é”®
    """
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model

    # Format the prompt
    current_date = get_today_str()
    formatted_prompt = answer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["research_brief"]),
        summaries="\n---\n\n".join(state["web_research_result"]),
    )

    # init Qwen Model, default to Qwen3-30B
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
    )
    result = llm.invoke(formatted_prompt)

    # Replace the short urls with the original urls and add all used urls to the sources_gathered
    unique_sources = []
    for source in state["sources_gathered"]:
        if source["short_url"] in result.content:
            result.content = result.content.replace(
                source["short_url"], source["value"]
            )
            unique_sources.append(source)

    return {
        "messages": [AIMessage(content=result.content)],
        "sources_gathered": unique_sources,
    }



async def save_file_node(state: OverallState, config: RunnableConfig) -> OverallState:
    """å°†ç ”ç©¶æŠ¥å‘Šä¿å­˜åˆ°æ–‡ä»¶çš„ LangGraph èŠ‚ç‚¹ã€‚

    Args:
        state: åŒ…å«è¿è¡Œæ‘˜è¦å’Œæ”¶é›†æ¥æºçš„å½“å‰å›¾çŠ¶æ€

    Returns:
        æŒ‡ç¤ºå·¥ä½œæµç»“æŸçš„å‘½ä»¤
    """
    # Implement file saving logic here
    configurable = Configuration.from_runnable_config(config)
    reasoning_model = state.get("reasoning_model") or configurable.answer_model

    # init Qwen Model, default to Qwen3-30B
    llm = ChatOpenAI(
        model=reasoning_model,
        temperature=0,
        max_retries=2,
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api-inference.modelscope.cn/v1"),
    )
    client = MultiServerMCPClient(
    {
        "GITHUB": {
            "url": os.getenv("GITHUB_TOKEN_KEY"),
            "transport": "sse"
        }

    }
)
    tools = await asyncio.wait_for(client.get_tools(), timeout=25)
    tools.append(save_file)
    agent_executor = create_react_agent(llm, tools, checkpointer=memory)

    try:
        # æ‰§è¡Œ RAG æ£€ç´¢
        save_prompt = f"""è¯·å°†ä¸‹é¢çš„å†…å®¹ä¿å­˜åˆ°knowledgeæ–‡ä»¶å¤¹ä¸­ï¼š{state['messages'][-1].content},ä¿å­˜æ–‡ä»¶åä¸º{state['research_brief'][:10]}.md,
        åŒæ—¶åœ¨æˆ‘çš„mydeepresearch_note githubä»“åº“ä¸­åˆ›å»ºç›¸åŒçš„æ–‡ä»¶
        """
        
        result = await agent_executor.ainvoke(
            {"messages": [HumanMessage(content=save_prompt)]},
        )
        
        # æå–ç»“æœ
        save_result = ""
        if result.get("messages"):
            save_result = result["messages"][-1].content
        
        return {
            "save_result": save_result,
        }
        
    except Exception as e:
        return {
            "save_result": f"æœ¬åœ°ä¿å­˜æ–‡ä»¶æ£€ç´¢å¤±è´¥: {str(e)}",
        }




def ask_save_permission(state: OverallState) -> Command[Literal["save_file", "__end__"]]:
    """å¤„ç†ç”¨æˆ·çš„ä¿å­˜å†³å®š"""
    state["awaiting_user_decision"] = True
    save_permission = interrupt({
        "question": "æ˜¯å¦ä¿å­˜æ–‡ä»¶?"
    })
    state["awaiting_user_decision"] = False
    if save_permission["save_permission"] == "save":
        return Command(goto="save_file", update={"save_permission": "save"})
    else:
        return Command(goto="__end__", update={"save_permission": "skip"})              


# Create our Agent Graph
builder = StateGraph(OverallState, config_schema=Configuration)

# Define all nodes in the workflow
builder.add_node("clarify_with_user", clarify_with_user)
builder.add_node("write_research_brief", write_research_brief)
builder.add_node("local_rag_search", local_rag_search)
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("finalize_answer", finalize_answer)
builder.add_node("ask_save_permission", ask_save_permission)
builder.add_node("save_file", save_file_node)
builder.add_node("finalize_answer_with_rag", finalize_answer_with_rag)

# æ–°çš„å·¥ä½œæµç¨‹ï¼š
# 1. é¦–å…ˆè¿›è¡Œæ„å›¾æ¾„æ¸…
builder.add_edge(START, "clarify_with_user")

builder.add_edge(
    "write_research_brief",
    "local_rag_search"
)

# 5. æœ¬åœ° RAG æ£€ç´¢ï¼ˆåŸæœ‰é€»è¾‘ä¿æŒä¸å˜ï¼‰
builder.add_conditional_edges(
    "local_rag_search", 
    evaluate_rag_result, 
    ["generate_query", "finalize_answer_with_rag"]
)

# 6. ç½‘ç»œæœç´¢æµç¨‹ï¼ˆåŸæœ‰é€»è¾‘ä¿æŒä¸å˜ï¼‰
builder.add_conditional_edges(
    "generate_query", continue_to_web_research, ["web_research"]
)
builder.add_edge("web_research", "reflection")
builder.add_conditional_edges(
    "reflection", evaluate_research, ["web_research", "finalize_answer"]
)

# 7. ç»“æŸèŠ‚ç‚¹
builder.add_edge("finalize_answer", "ask_save_permission")

builder.add_edge("save_file", END)
builder.add_edge("finalize_answer_with_rag", END)

graph = builder.compile(name="intent-clarification-rag-enhanced-search-agent",checkpointer=memory)

# ä¿å­˜å›¾å½¢ç»“æ„
try:
    png_data = graph.get_graph().draw_mermaid_png()
    with open("langgraph_structure.png", "wb") as f:
        f.write(png_data)
    print("âœ… å›¾å½¢ç»“æ„å·²ä¿å­˜ä¸º 'langgraph_structure.png'")
except Exception as e:
    print(f"âš ï¸  æ— æ³•ç”Ÿæˆå›¾å½¢: {e}")