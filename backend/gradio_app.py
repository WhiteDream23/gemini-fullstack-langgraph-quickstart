import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import gradio as gr
import asyncio
from langchain_core.messages import HumanMessage
from agent.graph import graph
import json
from typing import List, Dict, Any
import time


class ResearchAgent:
    def __init__(self):
        self.conversation_history = []
    
    def format_sources(self, sources: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–æœç´¢æ¥æº"""
        if not sources:
            return "æ— å¯ç”¨æ¥æº"
        
        formatted_sources = []
        for i, source in enumerate(sources, 1):
            title = source.get('title', 'æœªçŸ¥æ ‡é¢˜')
            url = source.get('value', source.get('url', ''))
            content_preview = source.get('content', '')[:200] + "..." if source.get('content') else ""
            
            formatted_sources.append(f"""
**æ¥æº {i}: {title}**
- é“¾æ¥: {url}
- æ‘˜è¦: {content_preview}
""")
        
        return "\n".join(formatted_sources)
    
    def research_query(self, question: str, initial_queries: int = 3, max_loops: int = 2, reasoning_model: str = "Qwen/Qwen3-30B-A3B-Instruct-2507"):
        """æ‰§è¡Œç ”ç©¶æŸ¥è¯¢"""
        if not question.strip():
            return "è¯·è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„é—®é¢˜ã€‚", "", ""
        
        try:
            # å‡†å¤‡è¾“å…¥çŠ¶æ€
            state = {
                "messages": [HumanMessage(content=question)],
                "initial_search_query_count": initial_queries,
                "max_research_loops": max_loops,
                "reasoning_model": reasoning_model,
            }
            
            # æ‰§è¡Œå›¾
            result = graph.invoke(state)
            
            # æå–ç»“æœ
            messages = result.get("messages", [])
            sources = result.get("sources_gathered", [])
            search_queries = result.get("search_query", [])
            
            # è·å–æœ€ç»ˆç­”æ¡ˆ
            final_answer = ""
            if messages:
                final_answer = messages[-1].content
            
            # æ ¼å¼åŒ–æ¥æº
            formatted_sources = self.format_sources(sources)
            
            # æ ¼å¼åŒ–æœç´¢æŸ¥è¯¢
            formatted_queries = ""
            if search_queries:
                formatted_queries = "æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢:\n" + "\n".join([f"â€¢ {query}" for query in search_queries])
            
            # ä¿å­˜åˆ°å¯¹è¯å†å²
            self.conversation_history.append({
                "question": question,
                "answer": final_answer,
                "sources": sources,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            return final_answer, formatted_sources, formatted_queries
            
        except Exception as e:
            error_msg = f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
            return error_msg, "", ""
    
    def get_conversation_history(self):
        """è·å–å¯¹è¯å†å²"""
        if not self.conversation_history:
            return "æš‚æ— å¯¹è¯å†å²"
        
        history_text = []
        for i, conv in enumerate(self.conversation_history, 1):
            history_text.append(f"""
**å¯¹è¯ {i} ({conv['timestamp']})**
**é—®é¢˜:** {conv['question']}
**å›ç­”:** {conv['answer'][:300]}{'...' if len(conv['answer']) > 300 else ''}
---
""")
        
        return "\n".join(history_text)


# åˆ›å»ºç ”ç©¶ä»£ç†å®ä¾‹
agent = ResearchAgent()


def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="Qwen æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .main-header {
            text-align: center;
            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 20px;
        }
        """
    ) as demo:
        
        gr.HTML("""
        <div class="main-header">
            ğŸ” Qwen æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹
        </div>
        <p style="text-align: center; font-size: 1.1em; color: #666;">
            åŸºäº Qwen æ¨¡å‹å’Œ Tavily æœç´¢çš„æ™ºèƒ½ç ”ç©¶å·¥å…·ï¼Œå¸®æ‚¨å¿«é€Ÿè·å–å‡†ç¡®ã€å…¨é¢çš„ä¿¡æ¯
        </p>
        """)
        
        with gr.Tab("ğŸ’­ æ™ºèƒ½é—®ç­”"):
            with gr.Row():
                with gr.Column(scale=2):
                    question_input = gr.Textbox(
                        label="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜",
                        placeholder="ä¾‹å¦‚ï¼šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—çš„æœ€æ–°å‘å±•ï¼Ÿ",
                        lines=3,
                        max_lines=5
                    )
                    
                    with gr.Row():
                        submit_btn = gr.Button("ğŸš€ å¼€å§‹ç ”ç©¶", variant="primary", size="lg")
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©º", variant="secondary")
                
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ é«˜çº§è®¾ç½®")
                    initial_queries = gr.Slider(
                        label="åˆå§‹æŸ¥è¯¢æ•°é‡",
                        minimum=1,
                        maximum=5,
                        value=3,
                        step=1,
                        info="ç”Ÿæˆçš„åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡"
                    )
                    max_loops = gr.Slider(
                        label="æœ€å¤§ç ”ç©¶å¾ªç¯",
                        minimum=1,
                        maximum=5,
                        value=2,
                        step=1,
                        info="æœ€å¤§åæ€å’Œè¡¥å……æœç´¢æ¬¡æ•°"
                    )
                    reasoning_model = gr.Dropdown(
                        label="æ¨ç†æ¨¡å‹",
                        choices=["Qwen/Qwen3-30B-A3B-Instruct-2507"],
                        value="Qwen/Qwen3-30B-A3B-Instruct-2507",
                        info="ç”¨äºåˆ†æå’Œæ¨ç†çš„æ¨¡å‹"
                    )
            
            gr.Markdown("---")
            
            with gr.Row():
                with gr.Column():
                    answer_output = gr.Textbox(
                        label="ğŸ“ ç ”ç©¶ç»“æœ",
                        lines=15,
                        max_lines=20,
                        show_copy_button=True
                    )
                
                with gr.Column():
                    with gr.Tab("ğŸ“š ä¿¡æ¯æ¥æº"):
                        sources_output = gr.Textbox(
                            label="æœç´¢æ¥æº",
                            lines=10,
                            max_lines=15,
                            show_copy_button=True
                        )
                    
                    with gr.Tab("ğŸ” æœç´¢è®°å½•"):
                        queries_output = gr.Textbox(
                            label="æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢",
                            lines=5,
                            max_lines=10
                        )
        
        with gr.Tab("ğŸ“œ å¯¹è¯å†å²"):
            history_display = gr.Textbox(
                label="å¯¹è¯è®°å½•",
                lines=20,
                max_lines=25,
                show_copy_button=True
            )
            refresh_history_btn = gr.Button("ğŸ”„ åˆ·æ–°å†å²è®°å½•", variant="secondary")
        
        with gr.Tab("â„¹ï¸ ä½¿ç”¨è¯´æ˜"):
            gr.Markdown("""
            ## ğŸŒŸ åŠŸèƒ½ç‰¹ç‚¹
            
            ### ğŸ¤– æ™ºèƒ½ç ”ç©¶
            - åŸºäº **Qwen3-30B** å¤§è¯­è¨€æ¨¡å‹
            - é›†æˆ **Tavily** å®æ—¶ç½‘ç»œæœç´¢
            - è‡ªåŠ¨ç”Ÿæˆå¤šè§’åº¦æœç´¢æŸ¥è¯¢
            - æ™ºèƒ½åˆ†æå’Œä¿¡æ¯æ•´åˆ
            
            ### ğŸ” æœç´¢èƒ½åŠ›
            - **å®æ—¶ç½‘ç»œæœç´¢**ï¼šè·å–æœ€æ–°ä¿¡æ¯
            - **å¤šæºéªŒè¯**ï¼šäº¤å‰éªŒè¯ä¿¡æ¯å‡†ç¡®æ€§
            - **æ™ºèƒ½ç­›é€‰**ï¼šè¿‡æ»¤é«˜è´¨é‡å†…å®¹
            - **æ¥æºè¿½æº¯**ï¼šæä¾›è¯¦ç»†å¼•ç”¨é“¾æ¥
            
            ### ğŸ’¡ ä½¿ç”¨æŠ€å·§
            1. **æ˜ç¡®é—®é¢˜**ï¼šé—®é¢˜è¶Šå…·ä½“ï¼Œç­”æ¡ˆè¶Šç²¾å‡†
            2. **è°ƒæ•´å‚æ•°**ï¼šæ ¹æ®éœ€è¦è°ƒæ•´æœç´¢æ·±åº¦
            3. **æŸ¥çœ‹æ¥æº**ï¼šéªŒè¯ä¿¡æ¯çš„å¯é æ€§
            4. **å¤šè½®å¯¹è¯**ï¼šåŸºäºå‰é¢ç»“æœç»§ç»­æ·±å…¥
            
            ### ğŸ“ ç¤ºä¾‹é—®é¢˜
            - "2024å¹´äººå·¥æ™ºèƒ½çš„æœ€æ–°çªç ´æ˜¯ä»€ä¹ˆï¼Ÿ"
            - "å¯å†ç”Ÿèƒ½æºæŠ€æœ¯çš„å‘å±•è¶‹åŠ¿å¦‚ä½•ï¼Ÿ"
            - "é‡å­è®¡ç®—åœ¨å®é™…åº”ç”¨ä¸­æœ‰å“ªäº›è¿›å±•ï¼Ÿ"
            - "åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èé¢†åŸŸçš„åº”ç”¨ç°çŠ¶"
            
            ### âš ï¸ æ³¨æ„äº‹é¡¹
            - ç¡®ä¿å·²é…ç½® `OPENAI_API_KEY` å’Œ `TAVILY_API_KEY`
            - æœç´¢ç»“æœåŸºäºç½‘ç»œå…¬å¼€ä¿¡æ¯
            - å»ºè®®éªŒè¯é‡è¦ä¿¡æ¯çš„å‡†ç¡®æ€§
            """)
        
        # äº‹ä»¶ç»‘å®š
        submit_btn.click(
            fn=agent.research_query,
            inputs=[question_input, initial_queries, max_loops, reasoning_model],
            outputs=[answer_output, sources_output, queries_output],
            show_progress=True
        )
        
        question_input.submit(
            fn=agent.research_query,
            inputs=[question_input, initial_queries, max_loops, reasoning_model],
            outputs=[answer_output, sources_output, queries_output],
            show_progress=True
        )
        
        clear_btn.click(
            fn=lambda: ("", "", "", ""),
            outputs=[question_input, answer_output, sources_output, queries_output]
        )
        
        refresh_history_btn.click(
            fn=agent.get_conversation_history,
            outputs=[history_display]
        )
    
    return demo


if __name__ == "__main__":
    demo = create_gradio_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        quiet=False
    )
